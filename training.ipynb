{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "from src.data import AudioDataset\n",
    "from src.models import AIGCNN\n",
    "from src.utils import STFT, iSTFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 22050\n",
    "WINLEN = 1024\n",
    "NFFT  = 1024\n",
    "SHIFT = 256\n",
    "\n",
    "NUM_FREQ = int(np.floor(NFFT/2))+1\n",
    "NUM_FRAMES = 101\n",
    "\n",
    "NUM_CH = 32\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 2\n",
    "LR = 4e-4\n",
    "SNR_MAX = 12.\n",
    "SNR_MIN = -6.\n",
    "\n",
    "SAVE_CHECKPOINT_NAME = \"checkpoints/new_checkpoint.pth\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "stft  = STFT(WINLEN, SHIFT, NFFT)\n",
    "istft = iSTFT(WINLEN, SHIFT, NFFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: AI-GCNN (amplitude-informed gated complex convolutional neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIGCNN(\n",
       "  (layer1): AIGCLayer(\n",
       "    (conv_re): Conv2d(3, 32, kernel_size=(3, 5), stride=(1, 1), padding=same, bias=False)\n",
       "    (conv_im): Conv2d(3, 32, kernel_size=(3, 5), stride=(1, 1), padding=same, bias=False)\n",
       "    (conv_gate): Conv2d(4, 32, kernel_size=(3, 5), stride=(1, 1), padding=same, bias=False)\n",
       "  )\n",
       "  (layer2): AIGCLayer(\n",
       "    (conv_re): Conv2d(32, 32, kernel_size=(3, 5), stride=(1, 1), padding=same, bias=False)\n",
       "    (conv_im): Conv2d(32, 32, kernel_size=(3, 5), stride=(1, 1), padding=same, bias=False)\n",
       "    (conv_gate): Conv2d(33, 32, kernel_size=(3, 5), stride=(1, 1), padding=same, bias=False)\n",
       "  )\n",
       "  (layer3): AIGCLayer(\n",
       "    (conv_re): Conv2d(32, 32, kernel_size=(3, 5), stride=(1, 1), padding=same, bias=False)\n",
       "    (conv_im): Conv2d(32, 32, kernel_size=(3, 5), stride=(1, 1), padding=same, bias=False)\n",
       "    (conv_gate): Conv2d(33, 32, kernel_size=(3, 5), stride=(1, 1), padding=same, bias=False)\n",
       "  )\n",
       "  (conv_re): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "  (conv_im): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "DNN = AIGCNN(NUM_CH, kernel_size=(3,5)).to(device)\n",
    "DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/'\n",
    "train_files = [os.path.join(data_folder, 'train', name) for name in os.listdir(os.path.join(data_folder, 'train'))]\n",
    "valid_files = [os.path.join(data_folder, 'valid', name) for name in os.listdir(os.path.join(data_folder, 'valid'))]\n",
    "test_files = [os.path.join(data_folder, 'test', name) for name in os.listdir(os.path.join(data_folder, 'test'))]\n",
    "    \n",
    "\n",
    "train_dataset = AudioDataset(train_files, stft)\n",
    "valid_dataset = AudioDataset(valid_files, stft)\n",
    "test_dataset = AudioDataset(test_files, stft)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_sensitive_mse(x_star, x_est):\n",
    "    return (torch.abs(x_star-x_est)**2).mean()\n",
    "\n",
    "optimizer = torch.optim.Adam(DNN.parameters(), LR)\n",
    "lr_scheduler = StepLR(optimizer, step_size=100, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/6250 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    ep_loss = 0\n",
    "    ep_val_loss = 0\n",
    "    for (X, amps) in tqdm(train_loader, desc='Training...'):\n",
    "        DNN.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Add gaussian noise\n",
    "        adn = np.random.randn(X.size(0), NUM_FREQ, NUM_FRAMES) + 1j*np.random.randn(X.size(0), NUM_FREQ, NUM_FRAMES)\n",
    "        norm_adn = np.linalg.norm(adn, ord='fro', axis = (1,2))\n",
    "        adn = adn / norm_adn[:, np.newaxis, np.newaxis] * 10.**(-1.*(np.random.rand()*(SNR_MAX-SNR_MIN)+SNR_MIN)/20)\n",
    "        noise = torch.tensor(adn, dtype=torch.complex64)\n",
    "        noise = noise*torch.linalg.norm(X, ord='fro', dim=[-2,-1], keepdim=True)\n",
    "\n",
    "        X = X.to(device)\n",
    "        amps = amps.to(device)\n",
    "        noise = noise.to(device)\n",
    "        X_noise = X + noise\n",
    "        \n",
    "        # Projections\n",
    "        Y = amps * torch.sgn(X_noise)\n",
    "        Z = stft(istft(Y))\n",
    "\n",
    "        # DNN\n",
    "        X_re = torch.real(X_noise).unsqueeze(1)\n",
    "        X_im = torch.imag(X_noise).unsqueeze(1)\n",
    "        Y_re = torch.real(Y).unsqueeze(1)\n",
    "        Y_im = torch.imag(Y).unsqueeze(1)\n",
    "        Z_re = torch.real(Z).unsqueeze(1)\n",
    "        Z_im = torch.imag(Z).unsqueeze(1)\n",
    "        feat_conc_re  = torch.cat([X_re, Y_re, Z_re], dim=1)\n",
    "        feat_conc_im  = torch.cat([X_im, Y_im, Z_im], dim=1)\n",
    "        out_dnn = DNN(feat_conc_re, feat_conc_im, amps)\n",
    "        X_out = Z - out_dnn\n",
    "\n",
    "        loss = phase_sensitive_mse(X, X_out)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(DNN.parameters(), max_norm=100.)\n",
    "        optimizer.step()\n",
    "\n",
    "        ep_loss += loss.item()\n",
    "\n",
    "    \n",
    "\n",
    "    for (X, amps) in tqdm(valid_loader, desc='Validating...'):\n",
    "        DNN.eval()\n",
    "        # Add noise\n",
    "        adn = np.random.randn(X.size(0), NUM_FREQ, NUM_FRAMES) + 1j*np.random.randn(X.size(0), NUM_FREQ, NUM_FRAMES)\n",
    "        norm_adn = np.linalg.norm(adn, ord='fro', axis = (1,2))\n",
    "        adn = adn / norm_adn[:, np.newaxis, np.newaxis] * 10.**(-1.*(np.random.rand()*(SNR_MAX-SNR_MIN)+SNR_MIN)/20)\n",
    "        noise = torch.tensor(adn, dtype=torch.complex64)\n",
    "        noise = noise*torch.linalg.norm(X, ord='fro', dim=[-2,-1], keepdim=True)\n",
    "\n",
    "        X = X.to(device)\n",
    "        amps = amps.to(device)\n",
    "        noise = noise.to(device)\n",
    "        X_noise = X + noise\n",
    "\n",
    "        # Projections\n",
    "        Y = amps * torch.sgn(X_noise)\n",
    "        Z = stft(istft(Y))\n",
    "\n",
    "        # DNN\n",
    "        X_re = torch.real(X_noise).unsqueeze(1)\n",
    "        X_im = torch.imag(X_noise).unsqueeze(1)\n",
    "        Y_re = torch.real(Y).unsqueeze(1)\n",
    "        Y_im = torch.imag(Y).unsqueeze(1)\n",
    "        Z_re = torch.real(Z).unsqueeze(1)\n",
    "        Z_im = torch.imag(Z).unsqueeze(1)\n",
    "        feat_conc_re  = torch.cat([X_re, Y_re, Z_re], dim=1)\n",
    "        feat_conc_im  = torch.cat([X_im, Y_im, Z_im], dim=1)\n",
    "        with torch.no_grad():\n",
    "            out_dnn = DNN(feat_conc_re, feat_conc_im, amps)\n",
    "            X_out = Z - out_dnn\n",
    "            valid_loss = phase_sensitive_mse(X, X_out)\n",
    "            \n",
    "        ep_val_loss += valid_loss.item()\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "    ep_loss = ep_loss/len(train_loader)\n",
    "    ep_val_loss = ep_val_loss/len(valid_loader)\n",
    "    train_losses.append(ep_loss)\n",
    "    valid_losses.append(ep_val_loss)\n",
    "    print(f'Epoch {ep+1}: Train loss = {ep_loss:.6f}, Validation loss = {ep_val_loss:.6f}')\n",
    "\n",
    "torch.save(DNN.state_dict(), SAVE_CHECKPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "degli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
